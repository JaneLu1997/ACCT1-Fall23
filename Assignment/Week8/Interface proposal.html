<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="Documentation.css" rel="stylesheet">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=DM+Sans:opsz@9..40&family=Playfair+Display&display=swap" rel="stylesheet">
    <title>Documentation: Digital Countdown</title>
</head>
<body>
    <header>
        <h1>Documentation: <br> Digital Countdown</h1>
    </header>
    <main>
        <div class="row">
            <img src="concept.png">
            <p>
                For this interface, I'm thinking about making a dice throwing device that's realized by Hand Pose in ml5js.
            </p>
        </div>

        <div class="row">
            <p>
                First, we need to flip the video also try to match drawn points to the image.
            </p>
            <img src="imageFixed.png">
        </div>

        <div class="row">
            <p>
                Second, we need to figure out if we can recognize gestures. After brief research, this is often achieved by TensorFlow. The premeter of this assignment is p5.js and ml5.js. However, this should be maneuvered by training the machine to know specific hand gestures based on the position of the points. As we can see, there's always 21 points registered for one hand.
            </p>
            <img src="pointLog.png">
        </div>

        <div class="row">
            <p>
                After this, to moniter what points we need exact locations in order to maneuver, we changed keypoints color in our code, and eventually we found out the keypoint data for the three fingers that we want to monitor: index fingertip (keypoint 9), middle fingertip (keypoint 13) and ring fingertip (keypoint 17)
            </p>
            <img src="keypointLocation.png">
        </div>
        <div class="row">
            <p>
                Additionally, we want to add another hand also as the point tracker. After some research, I'm looking into MediaPipe on how it is done with both hands.
            </p>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/BX8ibqq0MJU?si=LOeKAuPEZ6uhbqqT" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
        </div>

        <h1>Steps & Challenges</h1>
        <ul>
            <li>1. We need to have both hands tracked and shown on the video</li>
            <li>2. On top of that, we need to train the machine to recognize certain hand poses of number, six in total.</li>
            <li>3. We need to somehow train the machine to draw co-related images that's related to specific hand pose. How cand we do this? Is it doable?</li>
            <li>4. If we can achieve that, could it also work with both hands to draw a dice combo?</li>
        </ul>

        <h1>PROGRESS</h1>
        <iframe src="https://editor.p5js.org/jadedpineapple/full/0vkR1_qmw" width="1350" height="900"></iframe>

    </main>
</body>
</html>